Nous avons vu que les KGs apportent à la donnée un contexte sémantique nécessaire à une compréhension plus approfondie du fait, permettant la construction d'argumentaires plus élaborés. Cet argumentaire se base sur les chemins qui lient le sujet et l'objet du fait. Des approches complexes vont permettre d'obtenir des informations pertinentes sur les faits. Seulement les approches étudiées ne permettent pas d'aboutir à un système de fact checking efficace et rapide, qui permet une montée en charge importante des données. De plus cette approche est limitée aux faits de la forme d'un triple.

D'autres part, si on prend ClaimBuster et Credeye comme un système unique, ce système permettrait :
\begin{itemize}
    \item la récupération automatique de sources d'information (ClaimBuster)
    \item l'identification de fait vérifiables (ClaimBuster)
    \item la recherche d'un argumentaire sur le web (ClaimBuster et Credeye)
    \item une approche plus ou moins fiable de TALN pour la compréhension des données et des sources (ClaimBuster et Credeye)
\end{itemize}
Ce système serait capable d'effectuer toutes les étapes de fact-checking d'une fake news. Mais plusieurs problèmes se posent, le TALN n'est pas parfait, les sources du web ne sont pas fiables et les KGs disposent de trop peu d'informations. De plus une utilisation sophistiquée des KGs est coûteuse en temps.

L'idéal serait une base de connaissances qui permettent de récupérer les informations sur le web et puissent assurer la validité de l'information (voir couches Trust et Proof du web sémantique). C'est ce que Google tente de faire avec la base Knowledge Vault \cite{dong2014knowledge}. Ensuite il faut pouvoir utiliser cette base et obtenir des résultats dans un temps raisonnable et efficacement. Est-il possible de construire un algorithme de type machine learning qui s'entraînera sur des données complexes issues de bases de connaissances ? Le but serait de rassembler sur un même support les données complexes des KGs permettant de définir un contexte sémantique avec la capacité d'apprentissage du machine learning. Cela permettrait de palier au manque d'information, à la fiabilité de l'information et au manque de scalabilité des KGs. Nous verrons si le machine learning appliqué aux KGs peut apporter une réponse.

\subsection{Knowledge Vault}

Avant toute chose il faut pouvoir entraîner notre algo sur des donénes fiable et conséquentes.
\todo{finir}

Knowledge vault (KV) est une base de connaissance développée par Google. Elle s'inscrit dans la suite du Google Knowledge Graph et tente de l'étendre vers des sources d'information bien plus conséquentes. En effet KV a pour but de créer une base de connaissance à l'échelle du web \cite{dong2014knowledge}. Le problème des bases de connaissances est qu'elles reposent sur une action humaine pour être maintenues et enrichies. Leur évolution est lente et l'intégration de contenus récents fait défaut. De plus ces bases regroupent des faits génériques. Cette approche est trop chronophage pour produire du contenu spécifique fiable et en quantité. Pour répondre à ce problème, il est nécessaire d'automatiser les processus nécessaires à la construction d'une base de connaissances. Seulement cette approche revient à requêter les sources du web et appliquer du TALN. Ces 2 méthodes vont hériter des problèmes liés à l'incertitude de la donnée. Il est nécessaire de rendre la donnée fiable.

Comme nous l'avons précisé, les bases de connaissances peuvent déduire de l'information en se basant sur des ontologies. Si l'on récupère un fait qui nous dit que Barack Obama est né au Kenya, nous allons utiliser des faits antérieurs et connu pour vérifier cette affirmation. Et sachant que Barack Obama est président des États Unis, cette affirmation a beaucoup de chance d'être fausse. Pour combattre l'incertitude des sources de données disponibles sur le web il est nécessaire de dériver de la connaissance à partir des bases existantes. KV se construit autour des bases existantes (notamment Freebase et le Google Knowledge Graph) pour assurer la fiabilité des nouvelles données qu'il amasse. Il se base sur les 1,6 milliards de triplets dont il est composé; sachant que parmis ces triplets, 271 millions ont un indice de confiance de 0,9 ou plus.

Le potentiel de KV peut s'avérer être une source de données primordiales pour les systèmes de fact checking futures. L'idéal de KV serait une base de connaissances capable de s'auto-alimenter et s'auto-valider avec des sources de données hétérogènes et récentes.

\todo{finir}

\subsection{Machine learning et knowledge graph}

A voir pour cette partie : \cite{nickel2016review} \cite{gerber2015}  \cite{wilcke2017knowledge}

On peut se demander quel serait les tenants et aboutissants d'un algorithmes type machine learning entraînés avec les données issues de KV.